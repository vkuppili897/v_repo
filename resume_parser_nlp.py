# -*- coding: utf-8 -*-
"""Resume_Parser_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TaxFXu8iL9JCN6DcA85Z1tfYqi0CmK0o
"""

!pip install spacy
!pip install en_core_web_sm
# !python -m spacy download en_core_web_lg
!pip install pickle
# !pip install random
!pip install PyMuPDF
!pip install docxpy
!pip install exceptions
!pip install python-docx
!pip install PyPDF2

import spacy
from spacy import displacy
import pickle
import pandas as pd
import random
import sys, fitz
import numpy as np
import docxpy
import re
import collections
from collections import defaultdict
import csv
import docx
import pathlib
import PyPDF2
import string

nlp = spacy.load('en_core_web_sm')


# pdf = 'Resume_BG.pdf'

#Getting tech stack data

csv_file = 'bq-results-latest.csv'
tech_stack = defaultdict(list)

def get_tech_data(csv_file):
    terms = {'techstack' : [] }
    with open(csv_file) as f:
        reader = csv.DictReader(f)
            # print(reader)
        for row in reader:
            for (k, v) in row.items():
                    v = v.lower()
                    tech_stack[k].append(v)
            terms['techstack'] = tech_stack['title']
    
    return terms

get_tech_data(csv_file)



# print(terms)

#Converting docx to txt
def docx_reader(resume):
    doc = docx.Document(resume)
    fullText = []
    for para in doc.paragraphs:
        fullText.append(para.text)
    # print(fullText)
    return nlp('\n'.join(fullText).replace("\n", " ").lower())

#Converting pdf to txt
def pdf_reader(resume):
    doc = fitz.open(resume)
    text = ""
    for page in doc:
      text = text + str(page.getText())
    # print(text)
    #tx = "".join(text.split('\n'))
    tx = "".join(text.replace( "\n", " "  ))
    # tx += "".join(text)
    tx = tx.lower()
    return nlp(tx)




# doc = 'Resume.docx'
# pdf = 'Resume_KS.pdf'
# tx = pdf_reader(pdf)
# dx = docx_reader(doc)

# Getting tech stack data

sectors_file = 'sectors-latest.csv'
sectors_stack = defaultdict(list)

def get_sectors_data(sectors_file):
    sectors = {'domain' : [] }
    with open(sectors_file) as f:
        reader = csv.DictReader(f)
            # print(reader)
        for row in reader:
            for (k, v) in row.items():
                    v = v.lower()
                    sectors_stack[k].append(v)
            sectors['domain'] = sectors_stack['domain']
    
    return sectors

get_sectors_data(sectors_file)

def get_sectors(resume, return_object):

    text = get_text(resume)
    text_set = [word for word in text]
    sector_terms = get_sectors_data(sectors_file)
    sector_terms['domain'] = sectors_stack['domain']

    sectors = []      #unique list of sectors per resume
    sectors_path = []    #list of paths for unique sectors in resume
    detect_sectors = []     #list of sectors per resume
    # skill_data= []      #storing data labels for training NLP model
    detect_sectors = [str(word).lower().strip() for word in text_set if str(word) in [val for val_list in sector_terms.values() for val in val_list]]
    sector_data = (find_in_string(detect_sectors, str(text),"SECTOR"))

    # #Getting the unique values from detected sectors and storing in sectors
    for i in set(detect_sectors):
        sectors.append(i)
    # print(sectors)


    #Getting techstack paths for unique sectors and storing in sectors_path
    for x in sectors:
        if x in sectors_stack['domain']:
            i = sectors_stack['domain'].index(x)
            sectors_dict = {'sector' : sectors_stack['domain'][i],
                            'sector_path' :sectors_stack['sectorId'][i]
                            }
            sectors_path.append(sectors_dict)
            # sectors_path.append(sectors_stack['sectorId'][i])


    if (return_object == "detect_sectors"):
        return detect_sectors
    if (return_object == "sectors"):
        return sectors
    if (return_object == "sectors_path"):
        return sectors_path
    if (return_object == "sector_data"):
        return sector_data

    
# get_sectors('Resume_NW.pdf', "sectors")

# def get_sectors(resume, return_object):

#     text = get_text(resume)
#     text_set = [word for word in text]
#     sector_terms = get_sectors_data(sectors_file)
#     sector_terms['domain'] = sectors_stack['domain']

#     sectors = []      #unique list of sectors per resume
#     sectors_path = []    #list of paths for unique sectors in resume
#     detect_sectors = []     #list of sectors per resume
#     # skill_data= []      #storing data labels for training NLP model
#     detect_sectors = [str(word).lower().strip() for word in text_set if str(word) in [val for val_list in sector_terms.values() for val in val_list]]
#     sector_data = (find_in_string(detect_sectors, str(text),"SECTOR"))

#     # #Getting the unique values from detected sectors and storing in sectors
#     for i in set(detect_sectors):
#         sectors.append(i)
#     # print(sectors)


#     #Getting techstack paths for unique sectors and storing in sectors_path
#     for x in sectors:
#         if x in sectors_stack['domain']:
#             i = sectors_stack['domain'].index(x)
#             sectors_path.append(sectors_stack['sectorId'][i])

# # sectors_dict = {'sector' : sectors_stack['domain'][i],
# #                  'category_path' :sectors_stack['sectorId'][i]}

#     if (return_object == "detect_sectors"):
#         return detect_sectors
#     if (return_object == "sectors"):
#         return sectors
#     if (return_object == "sectors_path"):
#         return sectors_path
#     if (return_object == "sector_data"):
#         return sector_data

    
# # get_sectors('Resume_NW.pdf', "sectors")

# Getting tech stack data

categories_file = 'categories-latest.csv'
categories_stack = defaultdict(list)

def get_categories_data(categories_file):
    category_terms = {'category' : [] }
    with open(categories_file) as f:
        reader = csv.DictReader(f)
            # print(reader)
        for row in reader:
            for (k, v) in row.items():
                    v = v.lower()
                    categories_stack[k].append(v)
            category_terms['category'] = categories_stack['category']
    
    return category_terms

get_categories_data(categories_file)

def get_categories(resume, return_object):

    text = get_text(resume)
    text_set = [word for word in text]
    category_terms = get_categories_data(categories_file)
    category_terms['category'] = categories_stack['category']

    categories = []      #unique list of categories per resume
    categories_path = []    #list of paths for unique categories in resume
    detect_categories = []     #list of categories per resume
    # skill_data= []      #storing data labels for training NLP model
    detect_categories = [str(word).lower().strip() for word in text_set if str(word) in [val for val_list in category_terms.values() for val in val_list]]
    categories_data = (find_in_string(detect_categories, str(text),"CATEGORY"))

    # #Getting the unique values from detected categories and storing in categories
    for i in set(detect_categories):
        categories.append(i)
    # print(categories)


    #Getting techstack paths for unique categories and storing in categories_path
    for x in categories:
        if x in categories_stack['category']:
            i = categories_stack['category'].index(x)
            categories_dict = {'category' : categories_stack['category'][i],
                     'category_path' :categories_stack['category_id'][i]}
            categories_path.append(categories_dict)
            # categories_path.append(categories_stack['category_id'][i])

    # categors_dict = {'category' : categories_stack['category'][i],
                     
    #                  'category_path' :categories_stack['category_id'][i]}


    if (return_object == "detect_categories"):
        return detect_categories
    if (return_object == "categories"):
        return categories
    if (return_object == "categories_path"):
        return categories_path
    if (return_object == "categories_data"):
        return categories_data


get_categories('Resume_NW.pdf', "categories_data" )

def extract_text_from_pdf(resume):
    # Open pdf file
    pdfFileObj = open(resume,'rb')
    # Read file
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
    #print(type(pdfReader))
    # Get total number of pages
    num_pages = pdfReader.numPages
    # Initialize a count for the number of pages
    count = 0
    # Initialize a text empty etring variable
    text = ""
    # Extract text from every page on the file
    while count < num_pages:
        pageObj = pdfReader.getPage(count)
        count +=1
        text += str(pageObj.extractText())
    # Convert all strings to lowercase
    text = text.lower()
    text = text.translate(str.maketrans('','',string.punctuation))
    text = text.replace('\n', '')

    # text = text.replace('  ', ' ')

    return nlp(text)

# extract_text_from_pdf('Resume_BG.pdf')

def get_text(resume):
    if pathlib.PurePosixPath(resume).suffix == '.docx':
        text = docx_reader(resume)
        
    elif pathlib.PurePosixPath(resume).suffix == '.pdf':
        try: 
            text = pdf_reader(resume)
        except:
            text = extract_text_from_pdf(resume)

    return text

# print(get_text(pdf))

entities = []

# print(displacy.serve(doc, style="dep"))   #Relationship between each word - visualization

# #Type of each word

# for t in doc:
#   print(t, t.pos_, t.dep_, t.is_title)

# text = get_text(pdf)


def get_class(resume):
    text = get_text(resume)
    classify_matrix = []
    for ent in text.ents:               # .ents is the entity classifier
        classify = [str(ent.text), int(ent.start_char), int(ent.end_char), str(ent.label_)]
        classify_matrix.append(classify)
    return classify_matrix

# classify_matrix = get_class(pdf)
# print(classify_matrix)

# def tech_entity_tagger(resume, list):
#     tagged_entities = []
#     text = get_text(resume)
#     label = "TECH"
#     for i in list:
#         entity_search = re.search(str(i), str(text))
#         entity_tag = (int(entity_search.start(0)), int(entity_search.end(0)), label)
#         tagged_entities.append(entity_tag)
#     return tagged_entities

# entity_tagger(pdf, get_email(pdf))

#Getting email from txt


# text = get_text(pdf)

def get_email(resume, return_object):
    dev_email = []
    email_tags = []
    text = get_text(resume)
    text_set = [word for word in text]
    email = [email for email in text_set if re.match(r"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])", str(email))]

    for i in email : 
        email_match = re.search(str(i), str(text))
        if email_match:
            tag = (email_match.start(0), email_match.end(0), "EMAIL")
            email_tags.append(tag)
            # print(tag)   
    
    if (return_object == "email"):
        return email
    if (return_object == "email_tags"):
        return email_tags
    
    
    # return email






# get_email(pdf)

# print(entities)

# print(classify_matrix[3][3])

def correct_detail(resume, list, label):
    text = get_text(resume)
    initial_match = re.search(list[0], str(text))
    confirm_tag = ( )
    for i in list:
        print(i)
    confirm = input("Choose the right value or enter detail: ")
    confirm = confirm.lower().strip()
    confirm_match = re.search(confirm, str(text))
    print(confirm_match)
    #label = input("enter desired label: ")
    if confirm_match:
        confirm_tag = (confirm_match.start(0), confirm_match.end(0), label)
        list = [confirm]
    else:
        list = [confirm]
        confirm_tag = None
    
    if confirm_tag != None:
        
        return list, confirm_tag
    else:
        
        return list, confirm_tag

#Getting name using pre-trained model
def get_name(resume,return_object):
    text = get_text(resume)
    classify_matrix = get_class(resume)
    tags = []
    dev_names=[]

    for i in range(len(classify_matrix)):
        for j in range(len(classify_matrix[0])):
            if classify_matrix[i][j] == "PERSON":
                name = classify_matrix[i][0]
                tag = tuple(classify_matrix[i][1:4])
                tags.append(tag)
                dev_names.append(name)
            # else:
            #     name = "Developer"
            #     tag = (0, 0, "NONE")
            #     tags.append(tag)
            #     dev_names.append(name)
            # break
            
    # print(tags)
    # print("This is where the error is.")
    pre_tag = tags[0]
    # if pre_tag:
    #     return pre_tag
    # else:
    #     pre_tag = None

    pre_name = dev_names[0]
    # if pre_name:
    #     return pre_name
    # else:
    #     pre_name = None

    if (return_object == "dev_names"):
        return dev_names
    if (return_object == "pre_name"):
        return pre_name
    if (return_object == "pre_tag"):
        return pre_tag
    

#to get list, request get_name(resume, "dev_names")
#to get pre-name, request get_name(resume, 'pre_name')

# get_name(pdf, "pre_name")



#Getting name using pre-trained model
def get_locations(resume, return_object):
    text = get_text(resume)
    classify_matrix = get_class(resume)
    tags = []
    loc_names=[]

    for i in range(len(classify_matrix)):
        for j in range(len(classify_matrix[0])):
            if classify_matrix[i][j] == "GPE":
                name = classify_matrix[i][0]
                tag = tuple(classify_matrix[i][1:4])
                tags.append(tag)
                loc_names.append(name)
    pre_tag = tags[0]
    pre_loc = loc_names[0]

    # loc_names, confirm_tag = correct_detail(resume, loc_names)
    # print(confirm_tag)
    # return loc_names
    # print(pre_loc)


    if (return_object == "loc_names"):
        return loc_names
    if (return_object == "pre_loc"):
        return pre_loc
    if (return_object == "pre_tag"):
        return pre_tag



def find_in_string(tech_stacks: list, string_example: str, label: str) -> list:

    # Store the number of times we encounter each word.
    # This provides how many characters we remove with
    # replacement to find the word's start's index.
    word_count_dict = {}

    # This is the list of tuples which shall store the return values.
    indices = []

    # We iterate through each word passed in the tech stacks list.
    for word in tech_stacks:

        # If the word is new, we add it as having never been seen before in the dictionary,
        # With a value of 0.
        word_count_dict[word] = 0 if not word_count_dict.get(
            word) else word_count_dict[word]
        

        # In the temp variable, replace as many occurrences of the word as are stored
        # in the dictionary. This allows the find to find the instance we are looking for.
        temp_str = str.replace(string_example, word, "", word_count_dict[word])

        # The starting position is found.
        start = temp_str.find(word)

        # As other instances were removed, we have to supplement the starting index.
        addition = word_count_dict[word] * len(word)

        # Append the tuple of the word, its starting and ending indices, and the word "TECH".
        indices.append((start + addition, start +
                        addition + len(word), label))

        # Increment the word so that it can be removed if it comes up again.
        word_count_dict[word] += 1

    # Return the list of tuples.
    # print(word_count_dict)
    return indices


if __name__ == "__main__":

    tech_stacks = ["c++", "c++", "c++", "javascript",
                   "python", "github", "html", "go", "go"]
    string_example = "c++,github,html,go,c++,javascript,python,go,c++"

    print(find_in_string(tech_stacks, string_example, "label"))

def get_skills(resume, return_object):

    text = get_text(resume)
    text_set = [word for word in text]
    terms = get_tech_data(csv_file)
    terms['techstack'] = tech_stack['title']

    skills = []      #unique list of skills per resume
    skills_path = []    #list of paths for unique skills in resume
    detect_skills = []     #list of skills per resume
    # skill_data= []      #storing data labels for training NLP model
    detect_skills = [str(word).lower().strip() for word in text_set if str(word) in [val for val_list in terms.values() for val in val_list]]
    skill_data = (find_in_string(detect_skills, str(text),"TECH"))

    # #Getting the unique values from detected skills and storing in skills
    for i in set(detect_skills):
        skills.append(i)
    # print(skills)


    #Getting techstack paths for unique skills and storing in skills_path
    for x in skills:
        if x in tech_stack['title']:
            i = tech_stack['title'].index(x)
            skills_dict = {'skill' : tech_stack['title'][i],
                            'skill_path' : tech_stack['path'][i]}
            skills_path.append(skills_dict)
            # skills_path.append(tech_stack['path'][i])



    if (return_object == "detect_skills"):
        return detect_skills
    if (return_object == "skills"):
        return skills
    if (return_object == "skills_path"):
        return skills_path
    if (return_object == "skill_data"):
        return skill_data







results = []
def create_profile(resume, return_object):
    tags = []
    # classify_matrix = 
    text = get_text(resume)
    text_set = [word for word in text]
    classify_matrix = get_class(resume)
    dev_name = get_name(resume, "pre_name")
    dev_nametag = get_name(resume,"pre_tag")
    dev_emailtag = get_email(resume, "email_tags")
    dev_email = get_email(resume, "email")
    dev_loctag = get_locations(resume, "pre_tag")
    dev_loc = get_locations(resume, "pre_loc")
    pre_email = get_email(resume, "email")
    skills = get_skills(resume, "skills")
    skills_path = get_skills(resume, "skills_path")

    sectors = get_sectors(resume, "sectors")
    sectors_path = get_sectors(resume, "sectors_path")

    categories = get_categories(resume, "categories")
    categories_path = get_categories(resume, "categories_path")


    #appending skills to tags
    #MAYBE : append skills if yes, or request to enter skills and then append to tags
    for i in get_skills(resume, "skill_data"):
        tags.append(i)

    for i in get_sectors(resume, "sector_data"):
        tags.append(i)

    for i in get_categories(resume, "categories_data"):
        tags.append(i)
    
    dev_profile_dict = {'Name' : dev_name, 
                        'Location' : dev_loc , 
                        'Email' : dev_email,
                        'Domain' : {'Domain' : sectors,
                                    'SectorId' : sectors_path},
                        'TechStack' : {'Skills' : skills,
                                   'Skills Path' : skills_path},
                        'Category' : {'Categories' : categories,
                                      'Categories_path' : categories_path}
                         }
    print("Name : " + str(dev_name))
    print("Location : " + str(dev_loc))
    print("Email : " + str(dev_email))
    print("Domain: " + str(sectors))
    print("Skills : " + str(skills))
    print("Category : " + str(categories))

    print(dev_profile_dict)

    frontend = [{'skills_path' : skills_path},
                {'sectors_path' : sectors_path},
                {'category_path' : categories_path}
                ]


    print(frontend)

    if (return_object == "skills_path"):
        return skills_path
    if (return_object == "sectors_path"):
        return sectors_path
    if (return_object == "categories_path"):
        return categories_path

r_pdf = 'Resume_Java_RV.pdf'
r_docx = '/content/Resume_Fullstack_Rishabh_Bhaumik.docx'

create_profile(r_pdf, "skills_path")
#1. Output profile + tags including validation

#2. validations + Correct profile + =  retrain model.

"""NLP DATA COLLECTION"""



"""Custom NLP using spacy"""

#spacy.blank = creating a custom nlp model using spacy.
#specify the language for the model

# def custom_nlp_model(train_data):
nlp = spacy.blank('en')
# epoch = 10
if 'ner' not in nlp.pipe_names:   #A pipeline is a tagger
    ner = nlp.create_pipe('ner')    #Create a pipeline. ner = named entity recognition
    nlp.add_pipe(ner, last = True)

for _, annotation in train_data:   # for (   , {entities}) in training_data
    for ent in annotation['entities']:   # for each tuple in entitites (char start, char end , label)
        ner.add_label(ent[2])           #add ent[2] i.e. label as a label for the model to learn

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] # other pipes are not required. 
with nlp.disable_pipes(*other_pipes): #these are taggers (part of speech - Noun , Proper Noun, Adj) or tokenizers
    optimizer = nlp.begin_training()

for itn in range(101):            #defining number of iterations
    print("Starting iteration " + str(itn))   #Shuffling data to randomize data distribution
    random.shuffle(train_data)   #adding training data
    losses = {}     # empty dict to save losses from each iteration
    index = 0
    for text, annotations in train_data:    # train data = [(  'Resume' , { 'entities' : (pos, label) })]
    # For resume and entities in train_data
        try: 
            nlp.update(  #try updating the models knowledge by training with the new tags
                [text], #batch of texts
                [annotations], #batch of annotations or {enitites dict}
                drop = 0.2, #dropout - makes it harder to memorize
                sgd=optimizer, #callable to update weights
                losses = losses
            )
        except Exception as e:
            pass
        # print(text)

    print(losses)
#return 
print("Saving model to disk")
nlp.to_disk('/content/resume_nlp_model')
print("Saved")

train_data = pickle.load(open('train_data.pkl', 'rb'))

# new_nlp = custom_nlp_model(train_data)

nlp_model = spacy.load('/content/resume_nlp_model')

pdf = 'Resume_BG.pdf'
doc = nlp_model(str(get_text(pdf)))
doc2 = nlp_model(train_data[0][0])
y = []
for ent in doc.ents:
    labels = (ent.label_.upper() , ent.text)

    x = (f'{ent.label_.upper():{30}}- {ent.text}')
    y.append(labels)
    print(x)



